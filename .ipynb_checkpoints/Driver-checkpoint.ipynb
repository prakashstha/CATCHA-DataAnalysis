{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rootPath = '../../DATASET/RESULTS/features/clustering/'\n",
    "game_types = ['single/', 'merge/']\n",
    "days = ['day1/', 'day2/', 'day3/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clsesHead = [\"cls_gender\", \"cls_age2\", \"cls_age3\", \"cls_edu_highschool\", \"cls_edu_bachelor\",\n",
    "                 \"cls_edu_graduate\", \"cls_edu_all\", \"cls_prof_cs\", \"cls_prof_business\", \"cls_prof_all\"]\n",
    "\n",
    "featuresHead = [\"time\", \"time_first_touch\", \"time_first_action\", \"time_between_touches_mean\", \"time_between_touches_std\",\n",
    "     \"time_between_touches_min\", \"time_between_touches_max\", \"speed_touch_mean\", \"speed_touch_std\", \"speed_touch_min\",\n",
    "     \"speed_touch_max\", \"acc_touch_mean\", \"acc_touch_std\", \"acc_touch_min\", \"acc_touch_max\", \"real_distance_to_distance_mean\",\n",
    "     \"real_distance_to_distance_std\", \"real_distance_to_distance_min\", \"real_distance_to_distance_max\",\n",
    "     \"speed_move_mean\", \"speed_move_std\", \"speed_move_min\", \"speed_move_max\", \"acc_move_mean\", \"acc_move_std\",\n",
    "     \"acc_move_min\",\t\"acc_move_max\", \"move_real_distance_to_distance_mean\", \"move_real_distance_to_distance_std\",\n",
    "     \"move_real_distance_to_distance_min\", \"move_real_distance_to_distance_max\", \"distance_click_object_center_mean\",\n",
    "     \"distance_click_object_center_std\", \"distance_click_object_center_min\", \"distance_click_object_center_max\",\n",
    "     \"distance_drop_target_center_mean\", \"distance_drop_target_center_std\", \"distance_drop_target_center_min\",\n",
    "     \"distance_drop_target_center_max\", \"total_distance\", \"dif_time_stamp_mean\", \"dif_time_stamp_std\",\n",
    "     \"dif_time_stamp_min\", \"dif_time_stamp_max\", \"move_silence_mean\", \"move_silence_std\", \"move_silence_min\",\n",
    "     \"move_silence_max\", \"drag_silence_mean\", \"drag_silence_std\", \"drag_silence_min\", \"drag_silence_max\",\n",
    "     \"pause_and_drag_mean\", \"pause_and_drag_std\", \"pause_and_drag_min\", \"pause_and_drag_max\", \"pause_and_drop_mean\",\n",
    "     \"pause_and_drop_std\", \"pause_and_drop_min\", \"pause_and_drop_max\", \"angles_mean\", \"angles_std\", \"angles_min\", \"angles_max\"]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# listing the users from day 1\n",
    "daySelect = days[0]\n",
    "game = game_types[1]\n",
    "\n",
    "working_dir = rootPath + daySelect + game\n",
    "files_day1 = set([f for f in listdir(working_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareFeatures(inRoot, set_of_files):\n",
    "    features = pd.DataFrame(columns=featuresHead)\n",
    "    lbls = pd.DataFrame(columns=clsesHead)\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    for f in set_of_files:\n",
    "        cnt = cnt + 1\n",
    "        cur_file = inRoot + f\n",
    "        temp_df = pd.read_csv(cur_file)\n",
    "        \n",
    "        # select lbls only\n",
    "        temp_lbls = temp_df.loc[:, list(clsesHead)]\n",
    "\n",
    "        # dropping columns with classes\n",
    "        temp_df.drop(clsesHead, axis=1, inplace=True)\n",
    "\n",
    "        features = features.append(temp_df)\n",
    "        lbls = lbls.append(temp_lbls)\n",
    "#         print(features.shape)\n",
    "#         print(lbls.shape)\n",
    "#         print()\n",
    "    \n",
    "    return features, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_train, X_test, y_train, y_test):\n",
    "    # Re-sampling training data\n",
    "    sm = SMOTE(random_state=42)\n",
    "    print('original dataset %s' %Counter(y_train))\n",
    "    X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "    print('resampled dataset %s' %Counter(y_train_res))\n",
    "    y_train_res = y_train_res.flatten()\n",
    "    \n",
    "    # Feature normalization/scaling\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_train_res)\n",
    "    train_norm = sc.transform(X_train_res)\n",
    "    test_norm = sc.transform(X_test)\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    optimal_dimen = 35\n",
    "    pca = PCA(n_components = optimal_dimen)\n",
    "    pca.fit(train_norm)\n",
    "    train = pca.transform(train_norm)\n",
    "    test = pca.transform(test_norm)\n",
    "    \n",
    "    # build, fit and test the model\n",
    "#     rf = RandomForestClassifier(n_estimators = 400,\n",
    "#                                 max_depth = 40,\n",
    "#                                 max_features = 'log2',\n",
    "#                                 max_leaf_nodes = 170,\n",
    "#                                 min_samples_split = 0.30,\n",
    "#                                 bootstrap = False)\n",
    "    rf = SVC(C = 1, kernel = 'linear', probability = True)\n",
    "    rf.fit(train, y_train_res)\n",
    "    rf_predict = rf.predict(test)\n",
    "    return rf_predict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../DATASET/RESULTS/features/clustering/day2/merge/\n",
      "Day 2 Size: 62\n",
      "Before removal of file. Day 1 size: 104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# listing users from day 2\n",
    "daySelect = days[1]\n",
    "game = game_types[1]\n",
    "\n",
    "working_dir = rootPath + daySelect + game\n",
    "print(working_dir)\n",
    "files_day2 = set([f for f in listdir(working_dir)])\n",
    "print('Day 2 Size: %d' %len(files_day2))\n",
    "print('Before removal of file. Day 1 size: %d' % len(files_day1))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removal\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2EOOF9D135HQ1.csv\n",
      "1. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 18})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2FIANUBDARA16.csv\n",
      "2. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1085})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 20})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A21HSQ4CW72MNM.csv\n",
      "3. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 38})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A3AA5G6HENO6VJ.csv\n",
      "4. day 1 size: 103\n",
      "original dataset Counter({1.0: 1699, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1699, 1.0: 1699})\n",
      "Test Dataset Counter({2.0: 38, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A26RO8GGTQAXGG.csv\n",
      "5. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 56, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A382OV019QLW6H.csv\n",
      "6. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 74, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1PYODCHRF96S8.csv\n",
      "7. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1065})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 89, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A16UAN46CNIRHY.csv\n",
      "8. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1062})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 107, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A26T3M57NK46L5.csv\n",
      "9. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 120, 1.0: 16})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1YD6IATEIIG4X.csv\n",
      "10. day 1 size: 103\n",
      "original dataset Counter({1.0: 1699, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1699, 1.0: 1699})\n",
      "Test Dataset Counter({2.0: 120, 1.0: 32})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AJ6IIWWL0KF5Q.csv\n",
      "11. day 1 size: 103\n",
      "original dataset Counter({1.0: 1708, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1708, 1.0: 1708})\n",
      "Test Dataset Counter({2.0: 120, 1.0: 42})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AC2MEWTMN6G0Y.csv\n",
      "12. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 135, 1.0: 42})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2E6YPCJHJOLRD.csv\n",
      "13. day 1 size: 103\n",
      "original dataset Counter({1.0: 1699, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1699, 1.0: 1699})\n",
      "Test Dataset Counter({2.0: 135, 1.0: 59})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2TT6FAIWFD25N.csv\n",
      "14. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1063})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 153, 1.0: 59})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1AXTO77FG14E7.csv\n",
      "15. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({2.0: 153, 1.0: 77})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AAWX7QFQUNM6D.csv\n",
      "16. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({2.0: 153, 1.0: 95})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AGBDM85RLYTGL.csv\n",
      "17. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 171, 1.0: 95})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A26FQ2XNQB9RQG.csv\n",
      "18. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({2.0: 171, 1.0: 113})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A34NZ3C31CCJC.csv\n",
      "19. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({2.0: 171, 1.0: 130})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2G6074KBB11TH.csv\n",
      "20. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 189, 1.0: 130})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2VE5IV9OD2SK1.csv\n",
      "21. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1062})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 201, 1.0: 130})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1WF1X8PQQJ2OQ.csv\n",
      "22. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1063})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 212, 1.0: 130})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A379CH361AJDPD.csv\n",
      "23. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1070})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 130})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1XH05IKC77OXO.csv\n",
      "24. day 1 size: 103\n",
      "original dataset Counter({1.0: 1701, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1701, 1.0: 1701})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 147})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AOIR8V07FYMH5.csv\n",
      "25. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 165})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AFXEBGX1I0ZR3.csv\n",
      "26. day 1 size: 103\n",
      "original dataset Counter({1.0: 1702, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1702, 1.0: 1702})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 180})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A20CGE508BF5IE.csv\n",
      "27. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 197})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A118Y4ND7AFS2J.csv\n",
      "28. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({2.0: 226, 1.0: 213})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1MMC6X3ZNJ9OT.csv\n",
      "29. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1062})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({2.0: 243, 1.0: 213})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A21ZMR7O42OSMI.csv\n",
      "30. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({2.0: 243, 1.0: 230})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2JLU8XYHVY2ZP.csv\n",
      "31. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 248, 2.0: 243})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A110JYKWR9T6H8.csv\n",
      "32. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 265, 2.0: 243})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1D81U3G36XESE.csv\n",
      "33. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 282, 2.0: 243})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/ASX9Y47IOS1KR.csv\n",
      "34. day 1 size: 103\n",
      "original dataset Counter({1.0: 1718, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1718, 1.0: 1718})\n",
      "Test Dataset Counter({1.0: 299, 2.0: 243})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A37AJI03M37NPJ.csv\n",
      "35. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 317, 2.0: 243})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1B5O1E2T429ET.csv\n",
      "36. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1081})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 317, 2.0: 252})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1V1JNPU0KOA3X.csv\n",
      "37. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1067})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 317, 2.0: 266})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A33B85TN97HQ33.csv\n",
      "38. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1063})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 317, 2.0: 281})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A3L60SOWLYEDN2.csv\n",
      "39. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({1.0: 335, 2.0: 281})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/ABT4XCTQZJ0CP.csv\n",
      "40. day 1 size: 103\n",
      "original dataset Counter({1.0: 1700, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1700, 1.0: 1700})\n",
      "Test Dataset Counter({1.0: 352, 2.0: 281})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A33PDF03E6TRU0.csv\n",
      "41. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 370, 2.0: 281})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1V6BJT8KGVC1K.csv\n",
      "42. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1064})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 370, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1J4AZL6MZ675U.csv\n",
      "43. day 1 size: 103\n",
      "original dataset Counter({1.0: 1704, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1704, 1.0: 1704})\n",
      "Test Dataset Counter({1.0: 382, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1SWLR0UPOFRCW.csv\n",
      "44. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 400, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1TL8OAM2V1Q5K.csv\n",
      "45. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 417, 2.0: 299})\n",
      "Not in day 1: A3MHYBS6PHJ5QG.csv\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AXCPRXO08DFAH.csv\n",
      "46. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({1.0: 434, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2RDNYV4SH5E89.csv\n",
      "47. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 452, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A366JCV00ETS7V.csv\n",
      "48. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 470, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1FSDUZHTYIE4Y.csv\n",
      "49. day 1 size: 103\n",
      "original dataset Counter({1.0: 1701, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1701, 1.0: 1701})\n",
      "Test Dataset Counter({1.0: 486, 2.0: 299})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2WTDVHVVORNDU.csv\n",
      "50. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1084})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 486, 2.0: 304})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A130V7ERVRCTP4.csv\n",
      "51. day 1 size: 103\n",
      "original dataset Counter({1.0: 1703, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1703, 1.0: 1703})\n",
      "Test Dataset Counter({1.0: 504, 2.0: 304})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A3I9FTHSULHPYS.csv\n",
      "52. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1062})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 504, 2.0: 322})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A296H1EYAD611H.csv\n",
      "53. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 504, 2.0: 340})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AJFCCRG1TM3WT.csv\n",
      "54. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1061})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 504, 2.0: 357})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A1IMXNDHFHG2IE.csv\n",
      "55. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({1.0: 522, 2.0: 357})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A19TL7AJ0FB1JG.csv\n",
      "56. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({1.0: 540, 2.0: 357})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/ASYMMA0FAED7T.csv\n",
      "57. day 1 size: 103\n",
      "original dataset Counter({1.0: 1703, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1703, 1.0: 1703})\n",
      "Test Dataset Counter({1.0: 554, 2.0: 357})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A2ARHK50FQ79YC.csv\n",
      "58. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 557, 2.0: 357})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/ATK3DL2OEUYIH.csv\n",
      "59. day 1 size: 103\n",
      "original dataset Counter({1.0: 1727, 2.0: 1076})\n",
      "resampled dataset Counter({2.0: 1727, 1.0: 1727})\n",
      "Test Dataset Counter({1.0: 557, 2.0: 375})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/AGWB3ZV4DQI5Z.csv\n",
      "60. day 1 size: 103\n",
      "original dataset Counter({1.0: 1698, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1698, 1.0: 1698})\n",
      "Test Dataset Counter({1.0: 574, 2.0: 375})\n",
      "\n",
      "../../DATASET/RESULTS/features/clustering/day2/merge/A33FB8F5WQ0RUA.csv\n",
      "61. day 1 size: 103\n",
      "original dataset Counter({1.0: 1697, 2.0: 1091})\n",
      "resampled dataset Counter({2.0: 1697, 1.0: 1697})\n",
      "Test Dataset Counter({1.0: 592, 2.0: 375})\n"
     ]
    }
   ],
   "source": [
    "y_true = np.empty((0, 1), int)\n",
    "y_pred = np.empty((0, 1), int)\n",
    "\n",
    "cnt = 1;\n",
    "print('after removal')\n",
    "for f in files_day2:\n",
    "    if f in files_day1:\n",
    "        print()\n",
    "        print(working_dir + f)\n",
    "        test_features, test_lbls = prepareFeatures(working_dir, {f}) \n",
    "        \n",
    "        files_day1.remove(f)\n",
    "        print('%d. day 1 size: %d' % (cnt, len(files_day1)))\n",
    "        \n",
    "        train_features, train_lbls = prepareFeatures(rootPath + days[0] + game, files_day1)\n",
    "        \n",
    "        temp_y = test_lbls.cls_age2.values\n",
    "        temp_pred = predictUsingRF(train_features, test_features, train_lbls.cls_age2.values, test_lbls.cls_age2.values)\n",
    "        \n",
    "        \n",
    "        y_true = np.append(y_true, temp_y)\n",
    "        y_pred = np.append(y_pred, temp_pred)\n",
    "        \n",
    "        print('Test Dataset %s' %Counter(y_true))\n",
    "    \n",
    "        cnt = cnt + 1\n",
    "        files_day1.add(f)\n",
    "    else:\n",
    "        print('Not in day 1: ' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5389712154603039, 0.5367114788004137, 0.5377863098311711, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[363, 229],\n",
       "       [219, 156]], dtype=int64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../DATASET/features_all.csv')\n",
    "columnHeads = list(df.columns.values)\n",
    "featureHeads = list(df.columns.values[1:64])\n",
    "clsHeads = list(df.columns.values[64:])\n",
    "\n",
    "X_raw = df.loc[:, columnHeads]\n",
    "y = df.cls_gender.values\n",
    "y[y == 2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset Counter({1: 5054, 0: 3566})\n",
      "resampled dataset Counter({0: 5054, 1: 5054})\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "print('original dataset %s' %Counter(y))\n",
    "X_res, y_res = sm.fit_sample(X_raw, y)\n",
    "print('resampled dataset %s' %Counter(y_res))\n",
    "y_res = y_res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw, test_raw, train_labels, test_labels = train_test_split(X_res, y_res, \n",
    "                                                          stratify = y_res,\n",
    "                                                          test_size = 0.3, \n",
    "                                                          random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(train_raw)\n",
    "train = sc.transform(train_raw)\n",
    "test = sc.transform(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    " {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for roc_auc\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "1.000 (+/-0.000) for {'C': 1, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 10, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'kernel': 'linear'}\n",
      "1.000 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "1.000 (+/-0.000) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = 'roc_auc'\n",
    "print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "print()\n",
    "\n",
    "clf = GridSearchCV(SVC(probability = True), param_grid, cv=10,\n",
    "                   scoring='%s' % score)\n",
    "clf.fit(train, train_labels)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      1517\n",
      "          1       1.00      1.00      1.00      1516\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3033\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = test_labels, clf.predict(test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
